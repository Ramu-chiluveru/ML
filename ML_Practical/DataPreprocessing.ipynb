{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps :-\n",
    "##### 1. seperate features and labels from dataset\n",
    "##### 2. Handle missing values\n",
    "##### 3. encoding categorical data both independent and dependent\n",
    "##### 4. Splitting dataset into train & test sets\n",
    "##### 5. Feature Scaling only on training dataset\n",
    "feature scaling should be done on tranining dataset only that too after spliting. bcz, the the test should be realistic and should not be made contact or leaked to the model while training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Age        Salary\n",
      "count   9.000000      9.000000\n",
      "mean   38.777778  63777.777778\n",
      "std     7.693793  12265.579662\n",
      "min    27.000000  48000.000000\n",
      "25%    35.000000  54000.000000\n",
      "50%    38.000000  61000.000000\n",
      "75%    44.000000  72000.000000\n",
      "max    50.000000  83000.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/data_preprocessing.csv\")\n",
    "df.head()\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seperate the features from the dataset as x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# values method makes the indexed values as a np array\n",
    "x = df.iloc[:,:-1].values \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seperating the labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "y = df.iloc[:,-1].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## taking caring of missing values using sklearn.impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The SimpleImputer class in sklearn.impute is used to fill missing values in a dataset with a specified strategy, such as:\n",
    "\n",
    "Mean (mean) – replaces missing values with the column’s mean.\n",
    "Median (median) – replaces missing values with the column’s median.\n",
    "Most Frequent (most_frequent) – replaces missing values with the most common value.\n",
    "Constant (constant) – replaces missing values with a specified constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit() computes the mean of each column.\n",
    "### transform() fills missing values using the computed means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "#information about missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "\n",
    "# specify the only numerical columns\n",
    "\n",
    "#fit method calculates mean the for each targeted column\n",
    "imputer.fit(x[:,1:])\n",
    "\n",
    "# transform method replaces the missing values and returns the updated matrix\n",
    "x[:,1:] = imputer.transform(x[:,1:])\n",
    "print(x)\n",
    "\n",
    "# we can also fit_transform method \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning models work with numerical data only, meaning that categorical data (like colors, cities, or labels) must be converted into numbers before being used for training. Encoding categorical data ensures that:\n",
    "\n",
    "Models Understand Categorical Information – Algorithms like linear regression and decision trees can’t directly process text labels.\n",
    "Improves Model Performance – Correct encoding can enhance accuracy and make patterns easier to recognize.\n",
    "Avoids Bias in Ordinal Encoding – Some models may incorrectly assume order in categorical variables if not encoded properly.\n",
    "Handles Missing or Unseen Categories – Proper encoding ensures that missing or new values do not break the model.\n",
    "Types of Cate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding (OHE)\n",
    "Converts categories into binary columns.\n",
    "Ideal for nominal (unordered) categorical data. ex: [Red, Blue, Green ,Blue]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding (For Ordered Categories)\n",
    "If data has a natural order (e.g., small, medium, large), ordinal encoding is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Independent variables(features but\n",
    " not all features are independent)\n",
    " #### sklearn.compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Each Argument:\n",
    "\n",
    "#### transformers:\n",
    "#### A list of tuples, where each tuple contains:\n",
    "\n",
    "##### Name (str): A unique name for the transformer.\n",
    "##### Transformer (object or \"drop\" or \"passthrough\"): The transformation applied (e.g., StandardScaler(), OneHotEncoder(), \"drop\", or \"passthrough\").\n",
    "\n",
    "##### Columns (list, slice, str, or array-like): The columns to apply the transformer to.\n",
    "\n",
    "\n",
    "#### remainder (\"drop\", \"passthrough\", or estimator):\n",
    "\n",
    "How to handle columns that are not specified in transformers. Options:\n",
    "\n",
    "\"drop\" (default): Discards unused columns.\n",
    "\"passthrough\": Keeps unused columns unchanged.\n",
    "estimator: Apply a transformation to remaining columns.\n",
    "n_jobs (int, default=None):\n",
    "Specifies the number of jobs to run in parallel for transformations. None means single-threaded.\n",
    "\n",
    "verbose (bool, default=False):\n",
    "If True, will log the transformation process.\n",
    "\n",
    "verbose_feature_names_out (bool, default=True):\n",
    "If True, the feature names in the output are prefixed by the transformer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer  \n",
    "# Helps apply different transformations to different columns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder  \n",
    "# Converts categorical variables into numerical form (one-hot encoding)\n",
    "\n",
    "# for tranformers:- [(name,Transformer,[indexes of columns])]\n",
    "# for remainder:- \n",
    "ct = ColumnTransformer(transformers=[(\"encoder\",OneHotEncoder(),[0])],remainder=\"passthrough\")\n",
    "\n",
    "x = np.array(ct.fit_transform(x))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use labelEncoder for this variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset into train & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using sklearn.model_selection for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# it returns four sets of data two of x(test,train) and also for y\n",
    "\n",
    "# (independent,dependent,test_size,train_size,randome_state=1)\n",
    "# random_state = 1 selects the random data\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.2,train_size=0.8,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makes all the features on the same scale to prevent the domination of larger features on smaller features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# only numerical features\n",
    "# applying the standization\n",
    "\n",
    "#fit method calculates the mean and standard deviation\n",
    "# transform method applies the formula  = (xi - xmean)/s_std\n",
    "\n",
    "#fit_tranform method does both \n",
    "x_train[:,3:] = sc.fit_transform(x_train[:,3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for test set only perform the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:,3:] = sc.transform(x_test[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  [[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train: \" , x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test:  [[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_test: \" , x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
